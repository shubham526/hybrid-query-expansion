#!/usr/bin/env python3
"""
Learns optimal importance weights for query expansion using pre-computed features.

This script takes a feature file (generated by `create_training_data.py`) and
optimizes the weights (alpha, beta, gamma) for combining RM, BM25, and semantic
similarity scores. It uses a baseline run file to provide the candidate documents
for re-ranking during the optimization process.

Usage:
    # For a specific fold in a cross-validation setup
    python scripts/train_weights.py \
        --feature-file ./training_data_robust/features.json.gz \
        --validation-dataset disks45/nocr/trec-robust-2004 \
        --run-file-path ./bm25_baseline.txt \
        --query-ids-file ./folds/robust_fold_1_train.txt \
        --output-dir ./models_robust/fold1
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Callable, Tuple, List, Optional
import traceback

# Add project root to path for local imports
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import ir_datasets
from collections import defaultdict
from tqdm import tqdm

# Import your project's modules
from src.models.weight_optimizer import create_optimizer
from src.evaluation.evaluator import create_trec_dl_evaluator
from src.utils.file_utils import load_json, load_trec_run, save_learned_weights, ensure_dir
from src.utils.logging_utils import setup_experiment_logging, log_experiment_info, TimedOperation, \
    log_weight_optimization

logger = logging.getLogger(__name__)


def get_query_text(query_obj: Any) -> str:
    """
    Flexibly extracts query text from different ir_datasets query types.
    Handles MS MARCO (text) and TREC (title, description) formats.
    """
    if hasattr(query_obj, 'text'):
        return query_obj.text
    elif hasattr(query_obj, 'title'):
        if hasattr(query_obj, 'description') and query_obj.description:
            return f"{query_obj.title} {query_obj.description}"
        return query_obj.title
    else:
        logger.warning(f"Could not determine query text for query_id {query_obj.query_id}. Defaulting to empty string.")
        return ""


class MemoryEfficientWeightTrainer:
    """
    Memory-efficient version of WeightTrainer that processes documents in batches.
    """

    def __init__(self, reranker):
        """Initialize with memory-efficient reranker."""
        self.reranker = reranker
        logger.info("Memory-efficient WeightTrainer initialized")

    def create_evaluation_function(self,
                                   validation_data: Dict[str, Any],
                                   metric: str = "ndcg_cut_10") -> Callable[[Tuple[float, float, float]], float]:
        """Creates a memory-efficient evaluation function for the optimizer."""
        queries = validation_data['queries']
        qrels = validation_data['qrels']
        first_stage_runs = validation_data['first_stage_runs']
        features = validation_data['features']
        documents = validation_data['documents']

        # Prepare candidate documents in memory-efficient format
        candidate_docs_metadata = {}
        for qid, run in first_stage_runs.items():
            if qid in queries:
                candidate_docs_metadata[qid] = [
                    (doc_id, score) for doc_id, score in run
                ]

        logger.info("Memory-efficient evaluation function created (no pre-encoding)")

        def evaluate_weights(weights: Tuple[float, float, float]) -> float:
            """Memory-efficient evaluation function."""
            alpha, beta, gamma = weights
            reranked_runs = {}

            for qid, query_text in queries.items():
                if qid not in features or qid not in candidate_docs_metadata:
                    continue

                query_features = features[qid]

                # Compute importance weights
                importance_weights = {
                    term: (alpha * term_data['rm_weight'] +
                           beta * term_data['bm25_score'] +
                           gamma * term_data['semantic_score'])
                    for term, term_data in query_features['term_features'].items()
                }

                # Create expansion terms
                expansion_terms = [(term, term_data['rm_weight'])
                                   for term, term_data in query_features['term_features'].items()]

                # Prepare candidate results for this query
                candidate_results = []
                for doc_id, first_stage_score in candidate_docs_metadata[qid]:
                    doc_text = documents.get(doc_id, "")
                    if doc_text:  # Only include documents we have text for
                        candidate_results.append((doc_id, doc_text, first_stage_score))

                if not candidate_results:
                    # Fallback to first-stage ranking
                    reranked_runs[qid] = [(doc_id, score) for doc_id, score in candidate_docs_metadata[qid]]
                    continue

                try:
                    # Use streaming reranking to avoid memory issues
                    reranked_results = self.reranker.rerank_streaming(
                        query=query_text,
                        expansion_terms=expansion_terms,
                        importance_weights=importance_weights,
                        candidate_results=candidate_results,
                        top_k=1000  # Keep top 1000 for evaluation
                    )

                    reranked_runs[qid] = reranked_results

                except Exception as e:
                    logger.warning(f"Error reranking query {qid}: {e}")
                    # Fallback to first-stage ranking
                    reranked_runs[qid] = [(doc_id, score) for doc_id, score in candidate_docs_metadata[qid]]

            if not reranked_runs:
                logger.warning("No successful reranking results - returning 0.0")
                return 0.0

            try:
                evaluator = create_trec_dl_evaluator()
                evaluation = evaluator.evaluate_run(reranked_runs, qrels)
                score = evaluation.get(metric, 0.0)

                # Log memory usage periodically
                if hasattr(self.reranker, 'get_memory_stats'):
                    memory_stats = self.reranker.get_memory_stats()
                    logger.debug(f"Memory stats: {memory_stats}")

                logger.debug(f"Weights (α={alpha:.3f}, β={beta:.3f}, γ={gamma:.3f}) -> {metric}: {score:.4f}")
                return score

            except Exception as e:
                logger.warning(f"Error during evaluation: {e}")
                return 0.0

        return evaluate_weights

    def train(self,
              validation_data: Dict[str, Any],
              optimizer_type: str = "lbfgs",
              metric: str = "ndcg_cut_10") -> Tuple[float, float, float]:
        """Train weights with memory-efficient evaluation."""
        logger.info(f"Starting memory-efficient weight training with {optimizer_type}")

        # Log initial memory stats
        if hasattr(self.reranker, 'get_memory_stats'):
            initial_stats = self.reranker.get_memory_stats()
            logger.info(f"Initial memory stats: {initial_stats}")

        evaluation_function = self.create_evaluation_function(validation_data, metric)

        # Evaluate baseline
        with TimedOperation(logger, "Evaluating baseline performance"):
            baseline_weights = (1.0, 1.0, 1.0)
            baseline_score = evaluation_function(baseline_weights)
        logger.info(f"Baseline performance: {baseline_score:.4f}")

        # Optimize weights
        optimizer = create_optimizer(optimizer_type)

        with TimedOperation(logger, f"{optimizer_type.upper()} optimization"):
            optimal_weights = optimizer.optimize_weights(
                training_data=None,
                validation_queries=validation_data['queries'],
                validation_qrels=validation_data['qrels'],
                evaluation_function=evaluation_function
            )

        # Final evaluation
        with TimedOperation(logger, "Evaluating final weights"):
            final_score = evaluation_function(optimal_weights)

        # Log results and final memory stats
        log_weight_optimization(
            logger,
            initial_weights=baseline_weights,
            final_weights=optimal_weights,
            initial_score=baseline_score,
            final_score=final_score,
            iterations=getattr(optimizer, 'iterations', 'N/A')
        )

        if hasattr(self.reranker, 'get_memory_stats'):
            final_stats = self.reranker.get_memory_stats()
            logger.info(f"Final memory stats: {final_stats}")

        return optimal_weights


def validate_features(features: Dict[str, Any]) -> None:
    """Validate that features have the required structure and fields."""
    logger.info("Validating feature structure...")
    required_fields = ['rm_weight', 'bm25_score', 'semantic_score']

    if not features:
        raise ValueError("Features dictionary is empty")

    validated_queries = 0
    for qid, query_features in features.items():
        if 'term_features' not in query_features:
            raise ValueError(f"Missing 'term_features' for query {qid}")

        if not query_features['term_features']:
            logger.warning(f"Empty term_features for query {qid}")
            continue

        for term, term_data in query_features['term_features'].items():
            if not isinstance(term_data, dict):
                raise ValueError(f"term_data for term '{term}' in query {qid} is not a dictionary")

            missing_fields = [field for field in required_fields if field not in term_data]
            if missing_fields:
                raise ValueError(f"Missing fields {missing_fields} for term '{term}' in query {qid}")

            # Check that values are numeric
            for field in required_fields:
                try:
                    float(term_data[field])
                except (ValueError, TypeError):
                    raise ValueError(
                        f"Non-numeric value for {field} in term '{term}' of query {qid}: {term_data[field]}")

        validated_queries += 1

    logger.info(f"Feature validation passed! Validated {validated_queries} queries with required fields.")


def main():
    parser = argparse.ArgumentParser(
        description="Learn optimal query expansion weights using pre-computed features.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # Correctly add the run_file_path argument
    parser.add_argument('--feature-file', type=str, required=True, help='Path to the pre-computed feature file.')
    parser.add_argument('--validation-dataset', type=str, required=True, help='Name of the ir_datasets validation set.')
    parser.add_argument('--run-file-path', type=str,
                        help='Optional path to a baseline TREC run file for candidate generation.')
    parser.add_argument('--output-dir', type=str, required=True, help='Directory to save the learned weights.')
    parser.add_argument('--query-ids-file', type=str, default=None,
                        help='Optional path to a file with query IDs to use for training.')
    parser.add_argument('--semantic-model', type=str, default='all-MiniLM-L6-v2',
                        help='Sentence-transformer model for reranking.')
    parser.add_argument('--optimizer', type=str, default='lbfgs', choices=['lbfgs', 'grid', 'random'],
                        help='Optimization algorithm.')
    parser.add_argument('--metric', type=str, default='ndcg_cut_10', help='The retrieval metric to optimize.')
    parser.add_argument('--log-level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])

    args = parser.parse_args()

    output_dir = ensure_dir(args.output_dir)
    logger = setup_experiment_logging("train_weights", args.log_level, str(output_dir / 'weight_training.log'))
    log_experiment_info(logger, **vars(args))

    try:
        # --- Load and Filter Data ---
        with TimedOperation(logger, "Loading and filtering all data for training"):
            logger.info(f"Loading features from: {args.feature_file}")
            all_features = load_json(args.feature_file)

            # Validate features early
            validate_features(all_features)

            logger.info(f"Loading validation dataset: {args.validation_dataset}")
            dataset = ir_datasets.load(args.validation_dataset)

            all_queries = {q.query_id: get_query_text(q) for q in dataset.queries_iter()}

            all_qrels = defaultdict(dict)
            if dataset.has_qrels():
                for qrel in dataset.qrels_iter():
                    all_qrels[qrel.query_id][qrel.doc_id] = qrel.relevance

            # OPTIMIZED: Load documents on-demand instead of all at once
            logger.info("Setting up document access...")
            if hasattr(dataset, 'docs_count') and dataset.docs_count() > 100000:
                logger.info(
                    f"Large document collection detected ({dataset.docs_count():,} docs). Using on-demand loading.")

                # For large collections, create a lookup function
                def get_document_text(doc_id):
                    for doc in dataset.docs_iter():
                        if doc.doc_id == doc_id:
                            return doc.text if hasattr(doc, 'text') else doc.body
                    return ""

                all_documents = defaultdict(str)
                all_documents.default_factory = lambda: ""
            else:
                logger.info("Loading all documents into memory...")
                all_documents = {d.doc_id: (d.text if hasattr(d, 'text') else d.body)
                                 for d in tqdm(dataset.docs_iter(),
                                               total=dataset.docs_count() if hasattr(dataset, 'docs_count') else None,
                                               desc="Loading documents")}

            # --- Load candidate document runs ---
            all_runs = defaultdict(list)
            if dataset.has_scoreddocs():
                logger.info("Found 'scoreddocs' in dataset. Using them for candidate set.")
                for sdoc in dataset.scoreddocs_iter():
                    all_runs[sdoc.query_id].append((sdoc.doc_id, sdoc.score))
            elif args.run_file_path and Path(args.run_file_path).exists():
                logger.info(f"Using user-provided run file at '{args.run_file_path}' for candidate set.")
                all_runs.update(load_trec_run(args.run_file_path))
            else:
                raise ValueError(
                    "A source for candidate documents is required. Provide a run file via --run-file-path or use a dataset with scoreddocs.")

            # Filter to subset if specified
            queries_to_use = all_queries
            if args.query_ids_file:
                logger.info(f"Filtering data to subset from: {args.query_ids_file}")
                with open(args.query_ids_file, 'r') as f:
                    subset_qids = {line.strip() for line in f if line.strip()}

                queries_to_use = {qid: text for qid, text in all_queries.items() if qid in subset_qids}
                features_to_use = {qid: data for qid, data in all_features.items() if qid in subset_qids}
                qrels_to_use = {qid: data for qid, data in all_qrels.items() if qid in subset_qids}
                runs_to_use = {qid: data for qid, data in all_runs.items() if qid in subset_qids}
            else:
                features_to_use, qrels_to_use, runs_to_use = all_features, all_qrels, all_runs

            # Validate that we have data for the queries we want to train on
            missing_features = set(queries_to_use.keys()) - set(features_to_use.keys())
            missing_runs = set(queries_to_use.keys()) - set(runs_to_use.keys())

            if missing_features:
                logger.warning(f"Missing features for {len(missing_features)} queries: {list(missing_features)[:5]}...")
            if missing_runs:
                logger.warning(f"Missing runs for {len(missing_runs)} queries: {list(missing_runs)[:5]}...")

            # Only use queries where we have all required data
            valid_qids = (set(queries_to_use.keys()) &
                          set(features_to_use.keys()) &
                          set(runs_to_use.keys()) &
                          set(qrels_to_use.keys()))

            if len(valid_qids) < len(queries_to_use):
                logger.warning(f"Reduced from {len(queries_to_use)} to {len(valid_qids)} queries due to missing data")

            queries_to_use = {qid: queries_to_use[qid] for qid in valid_qids}
            features_to_use = {qid: features_to_use[qid] for qid in valid_qids}
            qrels_to_use = {qid: qrels_to_use[qid] for qid in valid_qids}
            runs_to_use = {qid: runs_to_use[qid] for qid in valid_qids}

            validation_data = {
                'features': features_to_use,
                'queries': queries_to_use,
                'qrels': dict(qrels_to_use),
                'documents': all_documents,
                'first_stage_runs': runs_to_use
            }

        logger.info(f"Successfully loaded and filtered data for {len(queries_to_use)} queries.")

        # --- Initialize Memory-Efficient Components ---
        logger.info(f"Initializing memory-efficient reranker with model: {args.semantic_model}")

        # Import the memory-efficient reranker
        from src.models.memory_efficient_reranker import create_memory_efficient_reranker

        # Determine if we expect large candidate sets
        total_candidates = sum(len(run) for run in runs_to_use.values())
        large_candidate_sets = total_candidates > 50000  # Threshold for "large"

        logger.info(f"Total candidates across all queries: {total_candidates:,}")
        logger.info(f"Using {'large' if large_candidate_sets else 'small'} candidate set optimization")

        reranker = create_memory_efficient_reranker(
            model_name=args.semantic_model,
            large_candidate_sets=large_candidate_sets
        )

        # Use memory-efficient trainer
        trainer = MemoryEfficientWeightTrainer(reranker=reranker)

        optimal_weights = trainer.train(
            validation_data=validation_data,
            optimizer_type=args.optimizer,
            metric=args.metric
        )

        # --- Save Final Weights ---
        weights_file_path = output_dir / 'learned_weights.json'
        save_learned_weights(
            weights=optimal_weights,
            filepath=weights_file_path,
            experiment_info=vars(args)
        )

        # Clear caches before exit
        if hasattr(reranker, 'clear_caches'):
            reranker.clear_caches()

    except Exception as e:
        logger.critical(f"Critical error during weight training: {e}", exc_info=True)
        sys.exit(1)

    logger.info("Weight training completed successfully!")
    logger.info(f"Optimal weights saved to: {weights_file_path}")


if __name__ == "__main__":
    main()