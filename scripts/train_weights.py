#!/usr/bin/env python3
"""
Learns optimal importance weights for query expansion using pre-computed features.

This script takes a feature file (generated by `create_training_data.py`) and
optimizes the weights (alpha, beta, gamma) for combining RM, BM25, and semantic
similarity scores. It uses a baseline run file to provide the candidate documents
for re-ranking during the optimization process.

Usage:
    # For a specific fold in a cross-validation setup
    python scripts/train_weights.py \
        --feature-file ./training_data_robust/features.json.gz \
        --validation-dataset disks45/nocr/trec-robust-2004 \
        --run-file-path ./bm25_baseline.txt \
        --query-ids-file ./folds/robust_fold_1_train.txt \
        --output-dir ./models_robust/fold1
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Callable, Tuple, List, Optional
import traceback

# Add project root to path for local imports
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import ir_datasets
from collections import defaultdict
from tqdm import tqdm

# Import your project's modules
from src.models.weight_optimizer import create_optimizer
from src.models.multivector_reranking import MultiVectorReranker
from src.evaluation.evaluator import create_trec_dl_evaluator
from src.utils.file_utils import load_json, load_trec_run, save_learned_weights, ensure_dir
from src.utils.logging_utils import setup_experiment_logging, log_experiment_info, TimedOperation, \
    log_weight_optimization

logger = logging.getLogger(__name__)

def get_query_text(query_obj: Any) -> str:
    """
    Flexibly extracts query text from different ir_datasets query types.
    Handles MS MARCO (text) and TREC (title, description) formats.
    """
    if hasattr(query_obj, 'text'):
        return query_obj.text
    elif hasattr(query_obj, 'title'):
        if hasattr(query_obj, 'description') and query_obj.description:
            return f"{query_obj.title} {query_obj.description}"
        return query_obj.title
    else:
        logger.warning(f"Could not determine query text for query_id {query_obj.query_id}. Defaulting to empty string.")
        return ""



class WeightTrainer:
    """
    Trains importance weights for query expansion using pre-computed features.
    """

    def __init__(self, reranker: MultiVectorReranker):
        """Initializes the WeightTrainer."""
        self.reranker = reranker
        logger.info("WeightTrainer initialized with a multi-vector reranker.")

    def create_evaluation_function(self,
                                   validation_data: Dict[str, Any],
                                   metric: str = "ndcg_cut_10") -> Callable[[Tuple[float, float, float]], float]:
        """Creates a lightweight evaluation function for the optimizer."""
        queries = validation_data['queries']
        qrels = validation_data['qrels']
        first_stage_runs = validation_data['first_stage_runs']
        features = validation_data['features']
        documents = validation_data['documents']

        candidate_docs_text = {}
        for qid, run in first_stage_runs.items():
            if qid in queries:  # Ensure we only process relevant queries
                candidate_docs_text[qid] = [(doc_id, documents.get(doc_id, ""), score) for doc_id, score in run]

        def evaluate_weights(weights: Tuple[float, float, float]) -> float:
            """The actual evaluation function passed to the optimizer."""
            alpha, beta, gamma = weights
            reranked_runs = {}

            for qid, query_text in queries.items():
                if qid not in features or not candidate_docs_text.get(qid):
                    continue

                query_features = features[qid]

                importance_weights = {
                    term: (alpha * term_data['rm_weight'] +
                           beta * term_data['bm25_score'] +
                           gamma * term_data['semantic_score'])
                    for term, term_data in query_features['term_features'].items()
                }

                reranked_results = self.reranker.rerank(
                    query=query_text,
                    expansion_terms=[(term, 0) for term in query_features['term_features'].keys()],
                    importance_weights=importance_weights,
                    candidate_results=candidate_docs_text[qid]
                )
                reranked_runs[qid] = reranked_results

            if not reranked_runs: return 0.0

            evaluator = create_trec_dl_evaluator()
            evaluation = evaluator.evaluate_run(reranked_runs, qrels)

            score = evaluation.get(metric, 0.0)
            logger.debug(f"Evaluated weights (α={alpha:.3f}, β={beta:.3f}, γ={gamma:.3f}) -> {metric}: {score:.4f}")
            return score

        return evaluate_weights

    def train(self,
              validation_data: Dict[str, Any],
              optimizer_type: str = "lbfgs",
              metric: str = "ndcg_cut_10") -> Tuple[float, float, float]:
        """Executes the full weight training pipeline."""
        logger.info(f"Starting weight training with optimizer '{optimizer_type}' to maximize '{metric}'")

        evaluation_function = self.create_evaluation_function(validation_data, metric)

        with TimedOperation(logger, "Evaluating baseline performance (weights=1,1,1)"):
            baseline_weights = (1.0, 1.0, 1.0)
            baseline_score = evaluation_function(baseline_weights)
        logger.info(f"Baseline performance with equal weights ({metric}): {baseline_score:.4f}")

        optimizer = create_optimizer(optimizer_type)
        with TimedOperation(logger, f"{optimizer_type.upper()} optimization"):
            optimal_weights = optimizer.optimize_weights(
                training_data=None,
                validation_queries=validation_data['queries'],
                validation_qrels=validation_data['qrels'],
                evaluation_function=evaluation_function
            )

        with TimedOperation(logger, "Evaluating final learned weights"):
            final_score = evaluation_function(optimal_weights)

        log_weight_optimization(
            logger,
            initial_weights=baseline_weights,
            final_weights=optimal_weights,
            initial_score=baseline_score,
            final_score=final_score,
            iterations=getattr(optimizer, 'iterations', 'N/A')
        )

        return optimal_weights


def main():
    parser = argparse.ArgumentParser(
        description="Learn optimal query expansion weights using pre-computed features.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # Correctly add the run_file_path argument
    parser.add_argument('--feature-file', type=str, required=True, help='Path to the pre-computed feature file.')
    parser.add_argument('--validation-dataset', type=str, required=True, help='Name of the ir_datasets validation set.')
    parser.add_argument('--run-file-path', type=str,
                        help='Optional path to a baseline TREC run file for candidate generation.')
    parser.add_argument('--output-dir', type=str, required=True, help='Directory to save the learned weights.')
    parser.add_argument('--query-ids-file', type=str, default=None,
                        help='Optional path to a file with query IDs to use for training.')
    parser.add_argument('--semantic-model', type=str, default='all-MiniLM-L6-v2',
                        help='Sentence-transformer model for reranking.')
    parser.add_argument('--optimizer', type=str, default='lbfgs', choices=['lbfgs', 'grid', 'random'],
                        help='Optimization algorithm.')
    parser.add_argument('--metric', type=str, default='ndcg_cut_10', help='The retrieval metric to optimize.')
    parser.add_argument('--log-level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])

    args = parser.parse_args()

    output_dir = ensure_dir(args.output_dir)
    logger = setup_experiment_logging("train_weights", args.log_level, str(output_dir / 'weight_training.log'))
    log_experiment_info(logger, **vars(args))

    try:
        # --- Load and Filter Data ---
        with TimedOperation(logger, "Loading and filtering all data for training"):
            logger.info(f"Loading features from: {args.feature_file}")
            all_features = load_json(args.feature_file)

            logger.info(f"Loading validation dataset: {args.validation_dataset}")
            dataset = ir_datasets.load(args.validation_dataset)

            all_queries = {q.query_id: get_query_text(q) for q in dataset.queries_iter()}

            all_qrels = defaultdict(dict)
            if dataset.has_qrels():
                for qrel in dataset.qrels_iter():
                    all_qrels[qrel.query_id][qrel.doc_id] = qrel.relevance

            all_documents = {d.doc_id: (d.text if hasattr(d, 'text') else d.body) for d in tqdm(dataset.docs_iter(), total=dataset.docs_count(), desc="Loading documents" )}

            # --- FIX: Implement the correct PRF source selection logic ---
            all_runs = defaultdict(list)
            if dataset.has_scoreddocs():
                logger.info("Found 'scoreddocs' in dataset. Using them for candidate set.")
                for sdoc in dataset.scoreddocs_iter():
                    all_runs[sdoc.query_id].append((sdoc.doc_id, sdoc.score))
            elif args.run_file_path and Path(args.run_file_path).exists():
                logger.info(f"Using user-provided run file at '{args.run_file_path}' for candidate set.")
                all_runs.update(load_trec_run(args.run_file_path))
            else:
                raise ValueError(
                    "A source for candidate documents is required. Provide a run file via --run-file-path or use a dataset with scoreddocs.")

            queries_to_use = all_queries
            if args.query_ids_file:
                logger.info(f"Filtering data to subset from: {args.query_ids_file}")
                with open(args.query_ids_file, 'r') as f:
                    subset_qids = {line.strip() for line in f if line.strip()}

                queries_to_use = {qid: text for qid, text in all_queries.items() if qid in subset_qids}
                features_to_use = {qid: data for qid, data in all_features.items() if qid in subset_qids}
                qrels_to_use = {qid: data for qid, data in all_qrels.items() if qid in subset_qids}
                runs_to_use = {qid: data for qid, data in all_runs.items() if qid in subset_qids}
            else:
                features_to_use, qrels_to_use, runs_to_use = all_features, all_qrels, all_runs

            validation_data = {
                'features': features_to_use,
                'queries': queries_to_use,
                'qrels': dict(qrels_to_use),
                'documents': all_documents,
                'first_stage_runs': runs_to_use
            }
        logger.info(f"Successfully loaded and filtered data for {len(queries_to_use)} queries.")

        # --- Initialize Components and Run Training ---
        reranker = MultiVectorReranker(model_name=args.semantic_model)
        trainer = WeightTrainer(reranker=reranker)

        optimal_weights = trainer.train(
            validation_data=validation_data,
            optimizer_type=args.optimizer,
            metric=args.metric
        )

        # --- Save Final Weights ---
        weights_file_path = output_dir / 'learned_weights.json'
        save_learned_weights(
            weights=optimal_weights,
            filepath=weights_file_path,
            experiment_info=vars(args)
        )

    except Exception as e:
        logger.critical(f"A critical error occurred during weight training: {e}", exc_info=True)
        sys.exit(1)

    logger.info("=" * 60 + "\nWEIGHT TRAINING SCRIPT COMPLETED SUCCESSFULLY\n" + "=" * 60)
    logger.info(f"Optimal weights saved to: {weights_file_path}")


if __name__ == "__main__":
    main()