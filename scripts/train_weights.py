#!/usr/bin/env python3
"""
Learns optimal importance weights for query expansion using pre-computed features.

This script takes a feature file (generated by `create_training_data.py`) and
optimizes the weights (alpha, beta, gamma) for combining RM, BM25, and semantic
similarity scores. The optimization is performed by maximizing a specified
retrieval metric (e.g., nDCG@10) on a validation set.

The core of this script is a lightweight evaluation function that uses the
pre-computed features, making the optimization process fast and efficient.

Usage:
    python scripts/train_weights.py \
        --feature_file ./training_data/msmarco-passage_trec-dl-2019_features.json.gz \
        --validation_dataset msmarco-passage/trec-dl-2019 \
        --output_dir ./models
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Callable, Tuple, List, Optional
import traceback

# Add project root to path for local imports
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import ir_datasets
from tqdm import tqdm

# Import your project's modules
from src.models.weight_optimizer import create_optimizer
from src.models.multivector_reranking import MultiVectorReranker
from src.evaluation.evaluator import create_trec_dl_evaluator
from src.utils.file_utils import load_json, save_learned_weights, ensure_dir
from src.utils.logging_utils import setup_experiment_logging, log_experiment_info, TimedOperation, \
    log_weight_optimization

logger = logging.getLogger(__name__)


class WeightTrainer:
    """
    Trains importance weights for query expansion using pre-computed features.
    """

    def __init__(self, reranker: MultiVectorReranker):
        """
        Initializes the WeightTrainer.

        Args:
            reranker: An instance of the MultiVectorReranker.
        """
        self.reranker = reranker
        logger.info("WeightTrainer initialized with a multi-vector reranker.")

    def create_evaluation_function(self,
                                   validation_data: Dict[str, Any],
                                   metric: str = "ndcg_cut_10") -> Callable[[Tuple[float, float, float]], float]:
        """
        Creates a lightweight evaluation function for the optimizer.

        This function uses pre-computed features to quickly evaluate the performance
        of a given set of weights (alpha, beta, gamma).

        Args:
            validation_data: A dictionary containing 'queries', 'qrels', 'first_stage_runs',
                             and the pre-computed 'features'.
            metric: The evaluation metric to optimize (e.g., 'ndcg_cut_10').

        Returns:
            A function that takes a weight tuple and returns a performance score.
        """
        queries = validation_data['queries']
        qrels = validation_data['qrels']
        first_stage_runs = validation_data['first_stage_runs']
        features = validation_data['features']
        documents = validation_data['documents']

        # Pre-load documents for candidates to speed up the loop
        candidate_docs_text = {}
        for qid, run in first_stage_runs.items():
            candidate_docs_text[qid] = []
            for doc_id, score in run:
                if doc_id in documents:
                    candidate_docs_text[qid].append((doc_id, documents[doc_id], score))

        def evaluate_weights(weights: Tuple[float, float, float]) -> float:
            """
            The actual evaluation function passed to the optimizer.
            """
            alpha, beta, gamma = weights
            reranked_runs = {}

            for qid, query_text in queries.items():
                if qid not in features or not candidate_docs_text.get(qid):
                    continue

                query_features = features[qid]

                # --- Fast Importance Calculation ---
                # This is the core of the efficient optimization.
                # We are just applying weights to pre-computed scores.
                importance_weights = {}
                for term, term_data in query_features['term_features'].items():
                    score = (alpha * term_data['rm_weight'] +
                             beta * term_data['bm25_score'] +
                             gamma * term_data['semantic_score'])
                    importance_weights[term] = score

                # --- Reranking ---
                # The reranker takes the candidates and the newly computed importance scores.
                reranked_results = self.reranker.rerank(
                    query=query_text,
                    expansion_terms=[(term, 0) for term in query_features['term_features'].keys()],
                    # rm_weight is not needed here
                    importance_weights=importance_weights,
                    candidate_results=candidate_docs_text[qid]
                )
                reranked_runs[qid] = reranked_results

            # --- Final Metric Computation ---
            if not reranked_runs:
                return 0.0

            evaluator = create_trec_dl_evaluator()  # Using default TREC DL metrics
            evaluation = evaluator.evaluate_run(reranked_runs, qrels)

            score = evaluation.get(metric, 0.0)
            logger.debug(f"Evaluated weights (α={alpha:.3f}, β={beta:.3f}, γ={gamma:.3f}) -> {metric}: {score:.4f}")
            return score

        return evaluate_weights

    def train(self,
              validation_data: Dict[str, Any],
              optimizer_type: str = "lbfgs",
              metric: str = "ndcg_cut_10") -> Tuple[float, float, float]:
        """
        Executes the full weight training pipeline.

        Args:
            validation_data: Dictionary with all required validation data components.
            optimizer_type: The optimization algorithm to use ('lbfgs', 'grid', 'random').
            metric: The metric to optimize.

        Returns:
            A tuple containing the optimal (alpha, beta, gamma) weights.
        """
        logger.info(f"Starting weight training with optimizer '{optimizer_type}' to maximize '{metric}'")

        # --- Create the lightweight evaluation function ---
        evaluation_function = self.create_evaluation_function(validation_data, metric)

        # --- Evaluate baseline performance (equal weights) ---
        with TimedOperation(logger, "Evaluating baseline performance (weights=1,1,1)"):
            baseline_weights = (1.0, 1.0, 1.0)
            baseline_score = evaluation_function(baseline_weights)
        logger.info(f"Baseline performance with equal weights ({metric}): {baseline_score:.4f}")

        # --- Run Optimization ---
        optimizer = create_optimizer(optimizer_type)
        with TimedOperation(logger, f"{optimizer_type.upper()} optimization"):
            # The optimizer will repeatedly call the fast evaluation_function
            optimal_weights = optimizer.optimize_weights(
                training_data=None,  # Not needed as features are pre-computed
                validation_queries=validation_data['queries'],
                validation_qrels=validation_data['qrels'],
                evaluation_function=evaluation_function
            )

        # --- Evaluate final performance ---
        with TimedOperation(logger, "Evaluating final learned weights"):
            final_score = evaluation_function(optimal_weights)

        log_weight_optimization(
            logger,
            initial_weights=baseline_weights,
            final_weights=optimal_weights,
            initial_score=baseline_score,
            final_score=final_score,
            iterations=getattr(optimizer, 'iterations', 'N/A')
        )

        return optimal_weights


def main():
    parser = argparse.ArgumentParser(
        description="Learn optimal query expansion weights using pre-computed features.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # --- Required Arguments ---
    parser.add_argument('--feature_file', type=str, required=True,
                        help='Path to the pre-computed feature file (e.g., features.json.gz).')
    parser.add_argument('--validation_dataset', type=str, required=True,
                        help='Name of the ir_datasets validation set (must match feature file).')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='Directory to save the learned weights.')

    # --- Model & Optimization Arguments ---
    parser.add_argument('--semantic_model', type=str, default='all-MiniLM-L6-v2',
                        help='Name of the sentence-transformer model used for reranking.')
    parser.add_argument('--optimizer', type=str, default='lbfgs', choices=['lbfgs', 'grid', 'random'],
                        help='Optimization algorithm to use.')
    parser.add_argument('--metric', type=str, default='ndcg_cut_10',
                        help='The retrieval metric to optimize.')

    # --- Logging Arguments ---
    parser.add_argument('--log_level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])

    args = parser.parse_args()

    # --- Setup ---
    output_dir = ensure_dir(args.output_dir)
    logger = setup_experiment_logging("train_weights", args.log_level, str(output_dir / 'weight_training.log'))

    log_experiment_info(logger, **vars(args))

    try:
        # --- Load All Necessary Data ---
        with TimedOperation(logger, "Loading all data for validation"):
            logger.info(f"Loading features from: {args.feature_file}")
            features = load_json(args.feature_file)

            logger.info(f"Loading validation dataset: {args.validation_dataset}")
            dataset = ir_datasets.load(args.validation_dataset)

            queries = {q.query_id: q.text for q in dataset.queries_iter()}
            qrels = {q.query_id: {d.doc_id: d.relevance for d in q.qrels_iter()} for q in dataset.queries_iter()}
            documents = {d.doc_id: d.text for d in dataset.docs_iter()}
            first_stage_runs = defaultdict(list)
            if dataset.has_scoreddocs():
                for sdoc in dataset.scoreddocs_iter():
                    first_stage_runs[sdoc.query_id].append((sdoc.doc_id, sdoc.score))

            validation_data = {
                'features': features,
                'queries': queries,
                'qrels': qrels,
                'documents': documents,
                'first_stage_runs': dict(first_stage_runs)
            }
        logger.info(f"Successfully loaded data for {len(queries)} queries.")

        # --- Initialize Components ---
        reranker = MultiVectorReranker(model_name=args.semantic_model)
        trainer = WeightTrainer(reranker=reranker)

        # --- Run Training ---
        optimal_weights = trainer.train(
            validation_data=validation_data,
            optimizer_type=args.optimizer,
            metric=args.metric
        )

        # --- Save Final Weights ---
        weights_file_path = output_dir / 'learned_weights.json'
        save_learned_weights(
            weights=optimal_weights,
            filepath=weights_file_path,
            experiment_info=vars(args)
        )

    except Exception as e:
        logger.critical(f"A critical error occurred during the weight training process: {e}")
        logger.critical(traceback.format_exc())
        sys.exit(1)

    logger.info("=" * 60)
    logger.info("WEIGHT TRAINING SCRIPT COMPLETED SUCCESSFULLY")
    logger.info(f"Optimal weights saved to: {weights_file_path}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()